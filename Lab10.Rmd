---
title: "NotTeamOnelab10"
author: "David Orozco, Ethan Schacht, Anderson Mun, Arie del Valle, Ryan Tate"
date: "March 20, 2019"
output: html_document
---

```{r,echo=FALSE, include=FALSE}
library(tidyverse)
library(lubridate)
library(stringr)
library(data.table)

Questions <- read.csv("Questions_trunc.csv")
Answers <- read.csv("answers_trunc.csv")

# merged <- left_join(Questions, Answers, by = c("Id" = "ParentId")) as NAs in date set

merged <- inner_join(Questions, Answers, by = c("Id" = "ParentId"))

```
```{r,echo=FALSE}
merged2 <- merged %>%
  mutate(question_title = Title, question_date = CreationDate.x, answer_date = CreationDate.y,
         question_score = Score.x, answer_score = Score.y,
         question_body = Body.x, answer_body = Body.y,
         question_id = Id, question_asker_id = OwnerUserId.x, question_answerer_id = OwnerUserId.y) %>%
  select(-CreationDate.x, -CreationDate.y, -Score.x, -Score.y, -Body.x, -Body.y, -Id, -X.x, -X.y, -Id.y, -OwnerUserId.x, -OwnerUserId.y, -Title)

merged3 <- merged2[c(8,9,1:2,4,6,10,3,5,7)]

merged3$question_date <- ymd_hms(merged3$question_date)
merged3$answer_date <- ymd_hms(merged3$answer_date)
```
****
####Team Section
****

Questions: 

* Is there a relationship between total time elapsed since a question was posted and the score of its answer?
* What is the relationship between the timeliness of an answer and its score? That is, does the first response to a question recieve a higher answer score than the following responses?
  

```{r,echo = FALSE}
dat <- merged3 %>% separate("question_date", into = c("date1","time1"), sep = " ") %>% separate("answer_date", into = c("date2","time2"), sep = " ") 
dat$date1 <- 

diff <- merged3 %>% mutate(diff = answer_date - question_date)
diff <- diff %>% mutate(relative = as.duration(diff)) %>% filter(diff > 0 )
dif <- diff %>% filter(diff < 1000)
#dif <- diff[,c(1:5,8,9,11,12)]%>% arrange(desc(question_id))
#loop <- unique(diff)

yote <- diff %>% mutate(sec = str_detect(relative, "s$"),min = str_detect(relative, "minute"),hour = str_detect(relative, "hour"), week = str_detect(relative, "week"),days = str_detect(relative, "day"), year = str_detect(relative, "year")) %>% arrange(-desc(diff))

s <- yote[c(1:45),]

m <- yote %>% filter(min == "TRUE") %>% mutate(min = diff/60) %>% arrange(desc(diff))
h <- yote %>% filter(hour == "TRUE")  %>% mutate(hour = diff/3600) %>% arrange(desc(diff))
d <- yote %>% filter(days == "TRUE")  %>% mutate(day = diff/(3600*24)) %>% arrange(desc(diff))
w <- yote %>% filter(week == "TRUE")  %>% mutate(week = diff/(3600*24*7)) %>% arrange(desc(diff))
y <- yote %>% filter(year == "TRUE")  %>% mutate(year = diff/(3600*24*365)) %>% arrange(desc(diff))

```

```{r,echo=FALSE,warning = FALSE,message=FALSE}

ggplot(data = m, mapping = aes(y = answer_score,x = min))+
  geom_point() + ggtitle("min") + ylim(0,1000) +
  ggtitle("Stackoverflow Threads: Timing of Response vs Answer Score") +
  xlab("Response Time (min)") +
  ylab("Answer Score")
ggplot(data = d, mapping = aes(y = answer_score,x = day))+
  geom_point() + ggtitle("day")+ ylim(0,1000) +
  ggtitle("Stackoverflow Threads: Timing of Response vs Answer Score") +
  xlab("Response Time (days)") +
  ylab("Answer Score")

```
```{r,echo=FALSE,include=FALSE}
ggplot(data = h, mapping = aes(y = answer_score,x = hour))+
  geom_point() + ggtitle("hour") + ylim(0,1000) 
ggplot(data = d, mapping = aes(y = answer_score,x = day))+
  geom_point() + ggtitle("day")+ ylim(0,1000) 
ggplot(data = w, mapping = aes(y = answer_score,x = week))+
  geom_point() + ggtitle("week") + ylim(0,1000) 
ggplot(data = y, mapping = aes(y = answer_score,x = year))+
  geom_point() + ggtitle("year")+ ylim(0,110) 
ggplot(data = s, mapping = aes(y = answer_score,x = diff))+
  geom_point() + ggtitle("sec")
```

Findings/Explanation: 

* We compared the time difference of questions and answers to the answer score. The graph shows that there was not a significant correlation between the two.  However, there appears to be a more slight negative correlation if we look at the time range of 0-60 minutes versus larger time ranges.  Therefore, we conclude that the time elapsed since a question was asked has little to no affect on the answer score. This could be because people might go through stackoverflow and choose which question they want to answer depending on the quality, not necessarily which one was posted first. 

```{r,echo=FALSE}
# stats try percentage of question answered within a minute, hour, day, week, year. and the amount of those
#ss <- s %>% summarize(score = mean(answer_score))
#mm <- m %>% summarize(score = mean(answer_score))
#hh <- h %>% summarize(score = mean(answer_score))
#dd <- d %>% summarize(score = mean(answer_score))
#ww <- w %>% summarize(score = mean(answer_score))
#yy <- y %>% summarize(score = mean(answer_score))


#sum(45,18801,9103,1511,2546,84)
stats <- tibble(
`Answered in...` = c("seconds","minutes","hours","days","weeks","years"),
amount = c(45,18801,9103,1511,2546,84),
percentage = c(45/32090,18801/32090,9103/32090,1511/32090,2546/32090,84/32090),
average_score = c(106,17.1,8.62,10.2,11.6,50.5),
`Score x Amount` = c(amount*average_score))

stats <- stats
stats

```

* The tibble above shows the amount of responses that respond within the range of second, minutes, etc. It also shows the percentage and average score of those time ranges. The final column in the tibble is the percent mulitplied by the score so we can get a relative score. It seems that most people respond within minutes(~58.6%). While other time ranges have a higher average score, minutes still have the highest relative score. This means that generally, the questions that are answered within minutes gernerally have better scores relatively.

```{r,echo = FALSE,message=FALSE}
# find all of the quickest response times for the same question Id and put a column with rank. 1 = fastest
yeet <- yote %>% group_by(question_id) %>% mutate(quickest = min(diff)) %>% arrange(desc(question_id))

jeepers <- yeet[,c(1,19)] %>% rename("diff" = "quickest")

yaawt <- semi_join(yeet,jeepers) %>% mutate(rank = 1) %>% arrange(desc(question_id)) 

# filters out all the rank one rows so the new "rank 1" is actually rank 2
yeet2 <- anti_join(yeet,jeepers) %>% group_by(question_id) %>% mutate(quickest = min(diff)) %>% arrange(desc(question_id))
# Ranks 2s
jeepers2 <- yeet2[,c(1,19)] %>% rename("diff" = "quickest")

yaawt2 <- semi_join(yeet2,jeepers2) %>% mutate(rank = 2) %>% arrange(desc(question_id)) 

# rank 3 filters out all ranks 2
yeet3 <- anti_join(yeet2,jeepers2) %>% group_by(question_id) %>% mutate(quickest = min(diff)) %>% arrange(desc(question_id))

jeepers3 <- yeet3[,c(1,19)] %>% rename("diff" = "quickest")

yaawt3 <- semi_join(yeet3,jeepers3) %>% mutate(rank = 3) %>% arrange(desc(question_id)) 

# rank 4 filters out all ranks 3
yeet4 <- anti_join(yeet3,jeepers3) %>% group_by(question_id) %>% mutate(quickest = min(diff)) %>% arrange(desc(question_id))

jeepers4 <- yeet4[,c(1,19)] %>% rename("diff" = "quickest")

yaawt4 <- semi_join(yeet4,jeepers4) %>% mutate(rank = 4) %>% arrange(desc(question_id)) 

# rank 5 filters out all ranks 4
yeet5 <- anti_join(yeet4,jeepers4) %>% group_by(question_id) %>% mutate(quickest = min(diff)) %>% arrange(desc(question_id))

jeepers5 <- yeet5[,c(1,19)] %>% rename("diff" = "quickest")

yaawt5 <- semi_join(yeet5,jeepers5) %>% mutate(rank = 5) %>% arrange(desc(question_id)) 
# rank 6 filters out all ranks 5
yeet6 <- anti_join(yeet5,jeepers5) %>% group_by(question_id) %>% mutate(quickest = min(diff)) %>% arrange(desc(question_id))

jeepers6 <- yeet6[,c(1,19)] %>% rename("diff" = "quickest")

yaawt6 <- semi_join(yeet6,jeepers6) %>% mutate(rank = 6) %>% arrange(desc(question_id)) 

```
****
* For these graphs and tibbles, we use a combination of `semi_join`,`full_join`, and "anti_join". First, we made a tibble that contain only the quickest answers per question Id and added a variable called rank which represents the order the answers came in. Rank 1 = first answer.
Then we used `semi-join` to keep all the questions in the orginial data that matched with the new tibble. This gives us a tibble with only rank 1 entries.
We repeated this process up to rank 6 then use full_join to combine all 6 ranks into one tibble.

```{r,echo=FALSE,message=FALSE}

ultimate_yeet <- full_join(yaawt,yaawt2) %>% full_join(.,yaawt3)%>% full_join(.,yaawt4)%>% full_join(.,yaawt5) %>% full_join(.,yaawt6)
```
```{r,echo=FALSE}
ggplot(data = ultimate_yeet, mapping = aes(x = rank))+
  geom_density(fill = "blue") + ggtitle("Distribution of Rank","Rank 'n' = nth responder")

n1 <- ultimate_yeet %>% group_by(rank) %>% count() %>% mutate(percent = n/30185)
n1


```


* The graph and tibble above shows the distribution of ranks. The graph shows a steady decrease in the denstiy of rank and the rank increases. 33% of all answers are rank 1 but only 18% are rank 3. This makes sense that there are more nth responses than (n+1)th responses. This is becuase the previous answers most likely sufficiently answered the question and there is no need to upload a new one.

****
```{r,echo=FALSE,message=FALSE}
ultimate_yeet <- ultimate_yeet %>% arrange(desc(question_id))

zoinks <- yote %>% group_by(question_id) %>% mutate(highscore = max(answer_score)) %>% arrange(desc(question_id))

shaggy <- zoinks[,c(1,19)] %>% rename("answer_score" = "highscore")

hawye <- semi_join(zoinks,shaggy) %>% arrange(desc(question_id)) 
#data below shows the rank for the answer with the highest answer score
force <- semi_join(ultimate_yeet,hawye)
```

* The graph and tibbles here were created in a similar manner has previously discussed. However we used `semi_join` to combine the data with all the ranks with a data set that contained only the top answer score for each question. This resulted in a tibble that contained only the top answer score per question and had a rank. We did this because we wanted to see which rank tends to have the highest score.

```{r,echo=FALSE}
 
ggplot(data = force, mapping = aes(x = rank))+
  geom_density(fill = "red") + ggtitle("Rank Distribution for Highest Answer Score","Rank 'n' = nth responder")
n2 <- force %>% group_by(rank) %>% count() %>% mutate(percent = n/10682)
n2
```

* The graph and tibble above shows the same trend has the Rank Distribution graph. This means that rank 1 answers tend to have the highest answer scores. Of all the top answers, 50% are rank 1. Only 27% are rank 2. This makes sense. There are many questions that only contain one rank, making Rank 1 the highest scoring by default. Also, there is a high chance that the first few question do a good job of answering the question and recieves a higher score.

****
* An overlay between the who distributions are shows below. This shows that while they follow the saem trend, the red is significatly higer for rank 1-3. This states that for rank 1-3, timeliness of answer does effect what rank recieve the highest score. 

```{r, echo=FALSE}

ggplot(data = force, mapping = aes(x = rank))+
  geom_density(fill = "red") + geom_density(data = ultimate_yeet, mapping = aes(x = rank),fill = "blue") + ggtitle("Difference in Distributions","Rank 'n' = nth responder")

#legend(5,1.5, legend=c("Line 1", "Line 2"),
       #col=c("red", "blue"))
n3 <- n1 %>% left_join(n2, by = "rank") %>% mutate( proportion = n.y/n.x) 
n3 <- n3[,c(1,6)]
n3
#n3 <- n3 %>% mutate(percent = diff/19503)


```

* The tibble above shows proportion = (rank n that had the highest answer score)/(all rank n). 52% of all rank 1 are the best answer. 36% of all rank 2 are the best answer. This shows that rank 1 have the highest proportion of top answers. 

**** 
#### Arie's Section
****
* Question: How does the average score of answers compare to the average score of questions that have to do with python?

* Explanation: I wanted to see if questions and answers that specified that they were talking about python had an effect on the overall score of the question and answer.

```{r, echo=FALSE}
questionT<- merged3%>%
  select(question_title,answer_score, question_score, question_date, answer_body)%>%
  group_by(question_title, question_date, answer_body)%>%
  summarise(avg_Qscore = mean(question_score), avg_Ascore = mean(answer_score))

titlesPython <- questionT%>%
  filter(str_detect(question_title, "python"))

answerPython <- questionT%>%
  filter(str_detect(answer_body, "python"))


# compares avg question score and answer score of quesition titles that are about python
ggplot(data = titlesPython, mapping = aes(x = avg_Qscore, y = avg_Ascore ))+
  geom_point()+geom_smooth(method = lm)+
  ggtitle("Question titles about python : Average question score vs. Average answer score")+
  xlab("Average Answer Score")+
  ylab("Average Question Score")

correlation1 <-cor(titlesPython$avg_Qscore, titlesPython$avg_Ascore )
correlation1

# compares avg quesiton score and answer score of anser titles that are about python
ggplot(data = answerPython, mapping = aes(x = avg_Qscore, y = avg_Ascore ))+
  geom_point()+geom_smooth(method = lm)+
  ggtitle("Answers about python: Average question score vs. Average answer score")+
  xlab("Average Answer Score")+
  ylab("Average Question Score")

correlation2 <- cor(answerPython$avg_Qscore, answerPython$avg_Ascore)
correlation2

```
* Process: First I created a variable that looks at specfic columns in the data. I used `select()` to focus on the columns of data that I wanted to use. I then used `group_by()` to gather all reapted question titles and answer body. After that I use `summarize()` to get the average of answer score and question score. Then I created two new variables, one that searchs for the word "python" using `str_detect()` in the question titles. The other varible also searchs for the word python in the answer bodyusing `str_detect()`. Then I made a `ggplot()` for both variables using `geom_point()` with a best line of fit, to see if there were any differences between question titles and answer body regarding the scores. To see if there was a correletion between the average score for answers and question titles I used `cor()` function.

* Findings: I found that there was not much of a difference bewteen the plots. The overall count of question titles compared to answers was higher, but the graphs display a similar relationship between question score and answer score. When calculating the correlation for the graphs, they both displayed a strong positive correlation around 0.5(correlations are underneath each plot). From these graphs, it shows that there is a relation between a question score and answer score. For example, a poorly scored question relates to a low scored answer. This makes sense, because the quality of a question will determine how someone is going to respond to it.

* Explanation: My guess was that answer and question scores would be higher overall becuase they were more specific. Taking everthing into account, if the answer body or question title included "python" in it didn't seem to have an and affect on the score. But I did find a relationship bewteen question and answer scores.

**** 
#### David's Section
****

* Question: How does the length of a question impact the average answer score for all questions with the same length.
* Explanation: This is important to investigate becuase there might be a correlation between specificity of a question and how well users can answer a question. 

```{r, echo=FALSE}
gut <- merged3 %>% group_by(question_id) %>% mutate(ave = mean(answer_score)) %>% mutate(length = str_length(question_title)) %>% arrange(desc(ave)) 
 
get <- gut[,c(1,3,11,12)]  %>% unique()  
 
ggplot(data = get, mapping = aes(x = length, y = ave))+ 
  geom_point() + ylab("Average Anwser Score ") + xlab("Character Length of Question") + ggtitle("Score Vs. Length")
```

* Process: I created a tibble that showed the average answer score for each unique question. I did this by using verbs such as `group_by` and `mutate` to create new variables. Within `mutate` I used `str_lenght` to count the amount of characters within a question title. Finally, I created a scatter plot using `geom_point` of this data to see if there was a coorelation between question length and answer score.

* Findings: My hypothesis was that there would be a optimum question length for highest answer scores. Linearly, there does not seem to be a coorelation bewteen length and score. I think this is becuase there are substantially more data points that are not outliers to the data. However, a majority of outliers (those questions wih really high answer scores) lie between 25-75 characters. This leads me to believe there is a optimum region where you can find the questions with the highest answer scores. 

* Explanantion: This makes sense. I believe this trend as to do with miscommunication. If a question is too short, it is often vague and very broad. When answering a broad question, there are many facets you can address. The answer may not be specifically what the reader was looking for, decreasing the average answer score. If a question is too long, it can be either too specific or too confusing. In this case, the person answering might misinterpret the question leading to an answer that does not help the user.

**** 
#### Ethan's Section
****

* Question: How does user question/answer politeness affect the score of their post?

* Explanation: I decided to pursue this question because I'm curious as to how people behave in response to polite words or lack thereof.

```{r, echo=FALSE}

polite <- merged3 %>%
  mutate(polite_question = ifelse(str_detect(question_body, "[T|t]hank you") | str_detect(question_body, "[T|t]hanks") |
         str_detect(question_body, "[P|p]lease") | str_detect(question_body, " [T|t]y "), "True", "False")) %>%
  mutate(polite_answer = ifelse(str_detect(answer_body, "[T|t]hank you") | str_detect(answer_body, "[T|t]hanks") |
         str_detect(answer_body, "[P|p]lease") | str_detect(answer_body, " [T|t]y "), "True", "False")) %>%
  group_by(polite_answer, polite_question, question_id) %>%
  summarise(avg_question_score = mean(question_score), avg_answer_score = mean(answer_score))

polite2 <- polite %>%
  group_by(polite_answer, polite_question) %>% #Another group_by required to deal with repeated question id's as a result of joining datasets
  summarise(avg_question_score = mean(avg_question_score), avg_answer_score = mean(avg_answer_score), count = n()) %>%
  unite(`polite_answer + polite_question`, polite_answer, polite_question, sep = " + ")

polite3 <- polite2[c(1,3,2,4)]

ggplot(data = polite3) +
  geom_point(mapping = aes(x = `polite_answer + polite_question`, y = avg_question_score, size = count), color = "blue") +
  geom_point(mapping = aes(x = `polite_answer + polite_question`, y = avg_answer_score, size = count), color = "green") +
  xlab("Polite Answer + Polite Question (True or False)") +
  ylab("Average Score") +
  ggtitle("Stackoverflow Threads: Politeness vs. Question/Answer Score", subtitle = "Blue = Question Score; Green = Answer Score")

```

* Process: I first created two new variables using ifelse(), having logical values of True or False depending upon my system's detection of common polite words.  I detected my answer bodies and question bodies for the words "please" and "thank you" and variations of these phrases to achieve this.  I then grouped by my two new variables and the question ID to get average question and answer scores for the 4 variations of "politeness."  Finally, I graphed the 4 variations of True/False against their repsective average question and answer scores, also taking their grouped counts into account using "size = count".

* Findings: My hypothesis was that polite questions and answers would receive higher average scores.  I was mostly proven wrong, especially when it came to average question scores as polite questions averaged scores of approxiately 12 (between False + True and True + True) and less polite questions averaged scores above 30! (between False + False and True + False). Also, when a polite question is "True", answer scores average barely over 6 whereas they average about 16 overwise.  It is notable that we have a much higher sample size for less polite questions, but these differences are still quite significant.  There doesn't appear to be as strong of a correlation between answer politeness and answer score itself, though the strength of an answer score consistently depends on the strength of the question score on average (Ex: A polite answer plus a less polite question yielded the highest average question score, as well as the highest average answer score).  

* Explanation: It is possible that more polite questions resulted in lower average question scores on StackOverflow because these types of questions reflect lack of knowledge on the subject as well as an undesirable informal approach to the complex subject of coding.  For example, people attempting to simply convince others to hand them code rather than show interest in more constructive help may feel more inclined to use words such as "please" or "thank you" in their questions.  Furthermore, average answer scores could correlate with average question scores because conversations may either go in a helpful direction or unhelpful direction on both ends.  Both the questioner and answerer can also both recieve the benefits of a popular thread, and both will also receieve the consequences of an unpopular thread.

**** 
#### Ryan's Section
****

##### string length of answer body vs answer score
```{r, echo=FALSE}


#get average score for each string length
boi <- merged3 %>% 
  group_by( str_length(answer_body)) %>%
  summarize(ave = mean(answer_score))

#got rid of some outliers messing up my graph >:(
boi1 <- boi %>%
  filter(ave < 750, `str_length(answer_body)` < 7500 )

#graph
ggplot(data = boi1, mapping = aes(x= `str_length(answer_body)`, y= ave)) +
  geom_point( alpha = 1/10) +
  geom_smooth( method='lm')


```


##### falsly postive word count in question body vs average score for question body   
```{r, echo=FALSE}



#make new column with count for the amount of "neat" words in each question body
merged4 <- merged3 %>%
  mutate(neat_count= str_count(question_body, c("neat", "neato","nice","noice")))

#get average score for each neat count
merged7 <-merged4 %>% 
  filter(neat_count < 3) %>% #neat count of 3 only happens once so it skews data
  group_by( neat_count) %>%
  summarize(ave = mean(question_score))

merged8 <-merged4 %>% 
  group_by( neat_count) %>%
  summarize(s = sum(question_score))

#graph
ggplot(data = merged7, mapping = aes(x = neat_count, y = ave)) +
  geom_point() +
  geom_smooth( method='lm')




```

* I created a feature called neat_count because I wanted to see the change in an questions score when they were positive, but maybe also slightly worried, which is displayed through the use of words like "neat", "neato","nice", and "noice".
* The relationship between question score and the count of worriedly positive words is slightly negative. This could be due to the fact that people who are worried about their post and try to remain positive are seen as being less honest than if they were just worried about getting an answer. The correlation isn't very strong though, and this could most likely be due to the fact that just becase someone used the word nice, it does not necessarily imply anything, whether about positivity or the level of worriedness.

* Also, I created a feature called str_length(answer_body) because I wanted to see the change in an answer's score when the answer became more extensive.
* The relationship between the length of the answer body and the score for the body is slightly positive, probably due to the fact that the more an answer is explained, the more likely the answer is right and thus will be upvoted. However, it is probably not more positive because some of the best answers are realtively simply and don't need a lot of words explaining them.

**** 
#### Anderson's Section
****

```{r, echo=FALSE}
How <- filter(merged3, str_detect(merged3$question_title, "how"))
#count(How)
What <- filter(merged3, str_detect(merged3$question_title, "what"))
#count(What)
When <- filter(merged3, str_detect(merged3$question_title, "when")) 
#count(When)
Where <- filter(merged3, str_detect(merged3$question_title, "where")) 
#count(Where)
pf <- tribble(
  ~Questionword, ~Number_of_questions, ~Average_times_asked,
  'How', 783, 0.0244,
  'What', 319, 0.009939,
  'When', 662, 0.020626,
  'Where', 92, 0.002866,
  'Everythingelse', 30240, 0.94217

)
pf

```

* I started out with creating several features listed as words that are frequently used in questions. The features included words like how, what, when and where. I used the actions filter and string detect to visualize the questions with the specified words. I also used a count function to retrieve the number of questions with said words in it. To show the results of my data I have created a tribble with the average times questions are asked with certain words in it.

* pf represents the tribble that gives you the data on the questions asked and their results by the numbers. The data in pf is very useful when trying to find out how many questions are asking the what, when, where and how. Through this data you can see that the questions asking how are the most frequent out of the specified words with 783 questions asking how. This number may seem large but when compared to everything else that didn't include these words there's only a 2.44% chance your question will ask how.

****
#### Who did what?

* David - Ind section, started GitKraken Repo, team aspect tibbles and graphs, finding/explanations for team section addressing the second question. 
* Arie - ind section and helped write findings for second question.
* Anderson- ind section
* Ethan- ind section and findings, merged and tidied/sorted the two datasets, findings/explanation on team section addressing first question. 
* Ryan- indiviudal section, researched data/time information for team section